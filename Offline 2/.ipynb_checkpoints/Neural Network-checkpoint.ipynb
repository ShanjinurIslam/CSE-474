{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(filename):\n",
    "    file = open(filename)\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for line in lines:\n",
    "        var = line.split()\n",
    "        X.append([float(x) for x in var[:-1]])\n",
    "        y.append(int(var[-1]))\n",
    "        \n",
    "    unique = len(np.unique(y))\n",
    "    \n",
    "    Y = []\n",
    "    for target in y:\n",
    "        instance = np.zeros(unique)\n",
    "        instance[target-1] = 1\n",
    "        Y.append(instance)\n",
    "        \n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = generate('trainNN.txt')\n",
    "X_test, Y_test  =  generate('testNN.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "class L_Layer_NN:\n",
    "    def __init__(self,L,k,learning_rate = 0.03):\n",
    "        self.L = L\n",
    "        self.k = k\n",
    "        self.W = []\n",
    "        self.W.append([])\n",
    "        self.lr = 0.03\n",
    "        self.k = np.insert(self.k,0,values=0,axis=0)\n",
    "        print(self.k)\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    def derivative(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def fit(self,X_train,Y_train,max_epoch=1000):\n",
    "        N = len(X_train)\n",
    "        \n",
    "        self.k[0] = X_train.shape[1]\n",
    "        \n",
    "        for r in range(1,self.L + 1):\n",
    "            self.W.append([])\n",
    "            self.W[r].append([])\n",
    "            for j in range(1,self.k[r]+1):\n",
    "                self.W[r].append([])\n",
    "                for k in range(self.k[r - 1] + 1):\n",
    "                    self.W[r][j].append(np.random.uniform(0, 1))\n",
    "                    \n",
    "        X_train = np.insert(X_train,0,values=1,axis=1)\n",
    "        Y_train = np.insert(Y_train,0,values=0,axis=1)\n",
    "        \n",
    "        for epoch in range(max_epoch):\n",
    "            y_m = []\n",
    "            \n",
    "            for i in range(N):\n",
    "                x_i = X_train[i]\n",
    "                \n",
    "                y = []\n",
    "                y.append(x_i)\n",
    "                \n",
    "                v = []\n",
    "                v.append([])\n",
    "                \n",
    "                for r in range(1,self.L+1):\n",
    "                    v_r = []\n",
    "                    v_r.append([])\n",
    "                    y_r = []\n",
    "                    y_r.append(1)\n",
    "                    for j in range(1,self.k[r]+1):\n",
    "                        dot_product = np.dot(np.array(self.W[r][j]),np.array(y[r-1]))\n",
    "                        v_r.append(dot_product)\n",
    "                        y_r.append(self.sigmoid(dot_product))\n",
    "                        \n",
    "                    v.append(v_r)\n",
    "                    y.append(y_r)\n",
    "                    \n",
    "                \n",
    "                delta_L = []\n",
    "                delta_L.append([])\n",
    "                \n",
    "                y_m.append(y[self.L])\n",
    "                \n",
    "                for j in range(1, self.k[self.L]+1):\n",
    "                    e_j = y[self.L][j] - Y_train[i][j]\n",
    "                    delta_L.append(e_j*self.derivative(v[self.L][j]))\n",
    "                    \n",
    "                \n",
    "                delta = []\n",
    "                delta.append([])\n",
    "                \n",
    "                for r in range(1,self.L+1):\n",
    "                    if r == self.L :\n",
    "                        delta.append(delta_L)\n",
    "                    else:\n",
    "                        temp = []\n",
    "                        temp.append([])\n",
    "                        for k in range(self.k[r]):\n",
    "                            temp.append(0)\n",
    "                        delta.append(temp)\n",
    "                        \n",
    "                \n",
    "                for r in range(self.L,1,-1):\n",
    "                    for j in range(1,self.k[r-1]+1):\n",
    "                        e_j = 0\n",
    "                        for k in range(1,self.k[r]+1):\n",
    "                            e_j += delta[r][k]*self.W[r][k][j]\n",
    "                        \n",
    "                        delta[r-1][j] = e_j*self.derivative(v[r-1][j])\n",
    "                \n",
    "                for r in range(1,self.L+1):\n",
    "                    for j in range(1,self.k[r]+1):\n",
    "                        del_w = np.multiply(y[r-1],delta[r][j])\n",
    "                        del_w = np.multiply(-self.lr,del_w)\n",
    "                        self.W[r][j] = self.W[r][j] + del_w\n",
    "            \n",
    "            \n",
    "            J = 0\n",
    "            for i in range(0, N):\n",
    "                e_i = 0.5*np.sum((y_m[i][1:]-Y_train[i][1:])**2,axis=0)\n",
    "                J += e_i\n",
    "            print(\"Epoch \" + str(epoch) + \", Cost \" + str(J))\n",
    "            if J < 10:\n",
    "                break\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self,X,Y):\n",
    "        X = np.insert(X,0,values=1,axis=1)\n",
    "        Y = np.insert(Y,0,values=0,axis=1)\n",
    "        \n",
    "        N = len(X)\n",
    "        true = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            x_i = X[i]\n",
    "                \n",
    "            y = []\n",
    "            y.append(x_i)\n",
    "                \n",
    "            for r in range(1,self.L+1):\n",
    "                y_r = []\n",
    "                y_r.append(1)\n",
    "                for j in range(1,self.k[r]+1):\n",
    "                    dot_product = np.dot(np.array(self.W[r][j]),np.array(y[r-1]))\n",
    "                    y_r.append(self.sigmoid(dot_product))\n",
    "                        \n",
    "                y.append(y_r)\n",
    "            \n",
    "            \n",
    "            if np.argmax(y[self.L][1:]) == np.argmax(Y[i][1:]):\n",
    "                true += 1\n",
    "                \n",
    "        return true/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 4]\n",
      "Epoch 0, Cost 301.15325014727136\n",
      "Epoch 1, Cost 187.78291160056423\n",
      "Epoch 2, Cost 185.72145264032113\n",
      "Epoch 3, Cost 184.20586948364297\n",
      "Epoch 4, Cost 182.42638809586734\n",
      "Epoch 5, Cost 180.24817636810383\n",
      "Epoch 6, Cost 177.51982952777465\n",
      "Epoch 7, Cost 174.06648831178538\n",
      "Epoch 8, Cost 169.7418077570964\n",
      "Epoch 9, Cost 164.5482947014756\n",
      "Epoch 10, Cost 158.74113181742248\n",
      "Epoch 11, Cost 152.75475379414027\n",
      "Epoch 12, Cost 147.00780401908617\n",
      "Epoch 13, Cost 141.7817112828238\n",
      "Epoch 14, Cost 137.20314786527607\n",
      "Epoch 15, Cost 133.27652111190204\n",
      "Epoch 16, Cost 129.93370323064045\n",
      "Epoch 17, Cost 127.07882693148764\n",
      "Epoch 18, Cost 124.6163284141515\n",
      "Epoch 19, Cost 122.46324304429453\n",
      "Epoch 20, Cost 120.55202277785419\n",
      "Epoch 21, Cost 118.82918438075716\n",
      "Epoch 22, Cost 117.2527296208307\n",
      "Epoch 23, Cost 115.78959641084482\n",
      "Epoch 24, Cost 114.41354547207148\n",
      "Epoch 25, Cost 113.1035316590511\n",
      "Epoch 26, Cost 111.84248937041136\n",
      "Epoch 27, Cost 110.61643762870447\n",
      "Epoch 28, Cost 109.41382003368996\n",
      "Epoch 29, Cost 108.22501305513522\n",
      "Epoch 30, Cost 107.0419542489367\n",
      "Epoch 31, Cost 105.85785732611826\n",
      "Epoch 32, Cost 104.6669930072013\n",
      "Epoch 33, Cost 103.46452325134982\n",
      "Epoch 34, Cost 102.24638193357399\n",
      "Epoch 35, Cost 101.00919769375761\n",
      "Epoch 36, Cost 99.75025503813866\n",
      "Epoch 37, Cost 98.46748853503935\n",
      "Epoch 38, Cost 97.15950286632875\n",
      "Epoch 39, Cost 95.82560928627476\n",
      "Epoch 40, Cost 94.46586731360112\n",
      "Epoch 41, Cost 93.08111972858198\n",
      "Epoch 42, Cost 91.67300949402146\n",
      "Epoch 43, Cost 90.24396918208839\n",
      "Epoch 44, Cost 88.79717671699498\n",
      "Epoch 45, Cost 87.33647531633666\n",
      "Epoch 46, Cost 85.8662598202462\n",
      "Epoch 47, Cost 84.39133547880195\n",
      "Epoch 48, Cost 82.91675817680256\n",
      "Epoch 49, Cost 81.4476666872571\n",
      "Epoch 50, Cost 79.98911779557908\n",
      "Epoch 51, Cost 78.54593418141098\n",
      "Epoch 52, Cost 77.12257308311209\n",
      "Epoch 53, Cost 75.72302135954149\n",
      "Epoch 54, Cost 74.35071995642565\n",
      "Epoch 55, Cost 73.00851828273963\n",
      "Epoch 56, Cost 71.6986568373786\n",
      "Epoch 57, Cost 70.4227747498204\n",
      "Epoch 58, Cost 69.18193778374551\n",
      "Epoch 59, Cost 67.97668180173171\n",
      "Epoch 60, Cost 66.80706664570937\n",
      "Epoch 61, Cost 65.67273575376687\n",
      "Epoch 62, Cost 64.57297748847733\n",
      "Epoch 63, Cost 63.50678497055115\n",
      "Epoch 64, Cost 62.472912081280924\n",
      "Epoch 65, Cost 61.469924127320816\n",
      "Epoch 66, Cost 60.49624238883877\n",
      "Epoch 67, Cost 59.550182361821506\n",
      "Epoch 68, Cost 58.62998594626735\n",
      "Epoch 69, Cost 57.73384813142838\n",
      "Epoch 70, Cost 56.859938906179586\n",
      "Epoch 71, Cost 56.00642120178674\n",
      "Epoch 72, Cost 55.17146568154561\n",
      "Epoch 73, Cost 54.35326315034395\n",
      "Epoch 74, Cost 53.55003528617113\n",
      "Epoch 75, Cost 52.76004430882208\n",
      "Epoch 76, Cost 51.98160210717229\n",
      "Epoch 77, Cost 51.21307924949227\n",
      "Epoch 78, Cost 50.452914201734444\n",
      "Epoch 79, Cost 49.69962297459495\n",
      "Epoch 80, Cost 48.951809308658675\n",
      "Epoch 81, Cost 48.20817538614216\n",
      "Epoch 82, Cost 47.46753292828913\n",
      "Epoch 83, Cost 46.72881440392943\n",
      "Epoch 84, Cost 45.99108394632766\n",
      "Epoch 85, Cost 45.25354746576644\n",
      "Epoch 86, Cost 44.51556137045873\n",
      "Epoch 87, Cost 43.77663928402945\n",
      "Epoch 88, Cost 43.0364561853569\n",
      "Epoch 89, Cost 42.294849499508395\n",
      "Epoch 90, Cost 41.55181683005833\n",
      "Epoch 91, Cost 40.80751022620041\n",
      "Epoch 92, Cost 40.06222709802636\n",
      "Epoch 93, Cost 39.31639810250482\n",
      "Epoch 94, Cost 38.57057249602534\n",
      "Epoch 95, Cost 37.82540156921038\n",
      "Epoch 96, Cost 37.081620838363165\n",
      "Epoch 97, Cost 36.34003166775337\n",
      "Epoch 98, Cost 35.60148294805525\n",
      "Epoch 99, Cost 34.86685337310067\n",
      "Epoch 100, Cost 34.137034754962855\n",
      "Epoch 101, Cost 33.41291670963969\n",
      "Epoch 102, Cost 32.69537294229105\n",
      "Epoch 103, Cost 31.985249268444463\n",
      "Epoch 104, Cost 31.28335342892344\n",
      "Epoch 105, Cost 30.590446692210875\n",
      "Epoch 106, Cost 29.907237187791328\n",
      "Epoch 107, Cost 29.2343748762254\n",
      "Epoch 108, Cost 28.572448034573323\n",
      "Epoch 109, Cost 27.921981117608954\n",
      "Epoch 110, Cost 27.28343384453638\n",
      "Epoch 111, Cost 26.6572013563149\n",
      "Epoch 112, Cost 26.04361528907248\n",
      "Epoch 113, Cost 25.442945613440028\n",
      "Epoch 114, Cost 24.8554030970866\n",
      "Epoch 115, Cost 24.281142257490416\n",
      "Epoch 116, Cost 23.720264683341775\n",
      "Epoch 117, Cost 23.17282261532666\n",
      "Epoch 118, Cost 22.638822689856486\n",
      "Epoch 119, Cost 22.118229762138927\n",
      "Epoch 120, Cost 21.61097073747558\n",
      "Epoch 121, Cost 21.11693835153888\n",
      "Epoch 122, Cost 20.635994851421255\n",
      "Epoch 123, Cost 20.16797553932867\n",
      "Epoch 124, Cost 19.712692149829483\n",
      "Epoch 125, Cost 19.26993603953768\n",
      "Epoch 126, Cost 18.8394811750197\n",
      "Epoch 127, Cost 18.421086910603808\n",
      "Epoch 128, Cost 18.014500552704416\n",
      "Epoch 129, Cost 17.61945971132601\n",
      "Epoch 130, Cost 17.235694442669033\n",
      "Epoch 131, Cost 16.862929189310904\n",
      "Epoch 132, Cost 16.500884526370644\n",
      "Epoch 133, Cost 16.14927872346652\n",
      "Epoch 134, Cost 15.807829133229843\n",
      "Epoch 135, Cost 15.476253417711067\n",
      "Epoch 136, Cost 15.154270624283715\n",
      "Epoch 137, Cost 14.84160212266529\n",
      "Epoch 138, Cost 14.537972414495727\n",
      "Epoch 139, Cost 14.243109826581108\n",
      "Epoch 140, Cost 13.95674709846426\n",
      "Epoch 141, Cost 13.67862187445759\n",
      "Epoch 142, Cost 13.408477109692177\n",
      "Epoch 143, Cost 13.146061399124934\n",
      "Epoch 144, Cost 12.891129237817752\n",
      "Epoch 145, Cost 12.643441220175884\n",
      "Epoch 146, Cost 12.402764185215203\n",
      "Epoch 147, Cost 12.168871314331538\n",
      "Epoch 148, Cost 11.941542187470283\n",
      "Epoch 149, Cost 11.72056280305278\n",
      "Epoch 150, Cost 11.505725566502104\n",
      "Epoch 151, Cost 11.29682925173331\n",
      "Epoch 152, Cost 11.093678939527742\n",
      "Epoch 153, Cost 10.896085936299883\n",
      "Epoch 154, Cost 10.70386767638695\n",
      "Epoch 155, Cost 10.516847610645188\n",
      "Epoch 156, Cost 10.334855083820491\n",
      "Epoch 157, Cost 10.15772520287378\n",
      "Epoch 158, Cost 9.985298698181664\n"
     ]
    }
   ],
   "source": [
    "output_neurons = Y_train.shape[1] \n",
    "\n",
    "model = L_Layer_NN(2,[4,output_neurons])\n",
    "\n",
    "min_max_scaler = MinMaxScaler()  # min max scaler\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.fit_transform(X_test)\n",
    "\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
