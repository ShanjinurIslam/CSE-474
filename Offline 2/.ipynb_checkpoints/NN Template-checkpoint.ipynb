{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler # important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 4) (500, 4)\n",
      "[1. 0. 0. 0.] [0.9, 0.7, 0.8, 98]\n"
     ]
    }
   ],
   "source": [
    "# train dataset generation\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "y = []\n",
    "\n",
    "file = open('trainNN.txt')\n",
    "lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    var = line.split()\n",
    "    X.append([float(x) for x in var[:-1]])\n",
    "    y.append(int(var[-1]))\n",
    "    \n",
    "unique = len(np.unique(y)) #unique \n",
    "\n",
    "\n",
    "for each in y:\n",
    "    arr = np.zeros(unique) # 1D array of 4 \n",
    "    arr[each-1] = 1 # 0 theke array indexing\n",
    "    Y.append(arr)\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "\n",
    "print(Y[0],[.9,.7,.8,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do same for test dataset\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Construction\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# write function for derivative\n",
    "def sigmoid_derivative(x):\n",
    "    # sigmoid(x)*(1-sigmoid(x))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer wise neuron count: [5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "\n",
    "input_layer_neurons = X.shape[1] # feature vector -> size\n",
    "output_layer_neurons = Y.shape[1] # output vector -> size\n",
    "L = 3 # Number of layers\n",
    "k = [input_layer_neurons,5,3,output_layer_neurons] # r => r -> koyta neuron ache\n",
    "learning_rate = 0.25 # \n",
    "threshold = 10 # error threshold\n",
    "N = len(X) # input vector\n",
    "\n",
    "print('Layer wise neuron count:',k[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08481967, 0.02337618, 0.03592103, 0.01542931],\n",
       "       [0.34304332, 0.33920722, 0.34411911, 0.33790838],\n",
       "       [0.91216182, 0.96628479, 0.9822458 , 0.97700922],\n",
       "       ...,\n",
       "       [0.03563476, 0.02700393, 0.02825983, 0.01263474],\n",
       "       [0.30462918, 0.34792384, 0.33300104, 0.33796701],\n",
       "       [0.94910985, 0.9700667 , 0.9795107 , 0.99051515]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# MinMaxScaler\n",
    "\n",
    "# src => https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_sample = scaler.fit_transform(X)\n",
    "scaled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it for X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weight vector\n",
    "\n",
    "Now we have to generate (n,m) shape weight vector for each layer\n",
    "\n",
    "Here, k = [4,5,3,4]\n",
    "\n",
    "So as L = 3 then shape of each layer weight vector would be like this,\n",
    "\n",
    "    W0 => (5,5)\n",
    "\n",
    "    W1 => (3,6)\n",
    "    \n",
    "    W2 => (4,4)\n",
    "    \n",
    "We will use numpy to generate our random 2D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1737581 , -4.01731447, -2.52609609, -3.56817342,  4.05676904],\n",
       "       [ 0.85287862, -4.26171427,  2.24020408,  3.43864165, -3.61950512],\n",
       "       [-0.37456143,  3.8333843 , -3.37960924, -4.14714734,  4.84066397],\n",
       "       [-4.89709964, -4.04732225,  2.37607004, -3.83526076, -3.15875887]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code to generate random 2D vector\n",
    "\n",
    "np.random.uniform(-5,5,size=(4,5)) # this will generate a shape of row->4 and column->5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weight vector for each layers\n",
    "W = []\n",
    "\n",
    "for r in range(0,L): \n",
    "    # do here , shape ashbe k array\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max epoch \n",
    "\n",
    "MAXEPOCH = 1000\n",
    "\n",
    "\n",
    "def forward_propagation(x_vector,target):\n",
    "    # 0 theke indexing hidden layer\n",
    "    \n",
    "    y = [] # per layer output \n",
    "    v = [] # per layer without sigmoid\n",
    "    \n",
    "    for r in range(L):\n",
    "        # calculate vr and yr for each layer\n",
    "        # vr ? np.dot( weight of layer r, x_vector) hidden layer 1 => 5 // []\n",
    "        # yr ? sigmoid(vr) // prothom e 1 append kore dite hobe\n",
    "        # append vr to v and yr to y = wTx + b\n",
    "        # y.append(yr)\n",
    "        # v.append(vr)\n",
    "        pass\n",
    "    \n",
    "    return v,y\n",
    "    \n",
    "def back_propagation(v,y):\n",
    "    # calculate last layer delta \n",
    "    # delta L j ( i ) = e j ( i ) f ' ( v j L ( i ) ) # page 166\n",
    "    # here j starts from 1 to k[L] and j represents 'j' th neuron in layer 'r'\n",
    "    \n",
    "    deltaL = 0 # complete this\n",
    "    \n",
    "    # x -> [a b c d]\n",
    "    # y = [0.9 0.8 0.7 0.6] Y = [1 0 0 0]\n",
    "    # e(i) = y-Y = [-0.1,0.8,0.7,0.6]\n",
    "    # v[L] = [0.89, 0.98,0.09,0.8]\n",
    "    # sigmoid_derivative(v[L]) => [0.99 0.98 0.97 0.87]\n",
    "    # delta L = [-0.1*.99 0.8*.98 0.7*.97 0.87*0.6 ]\n",
    "    # deltaL = np.multiply(y-Y,sigmoid_derivative(v[L]))\n",
    "    \n",
    "    # array delta will have a size of L ( number of layers )\n",
    "    \n",
    "    deltas = []\n",
    "    \n",
    "    #W0,W1,W2\n",
    "    #W2 => delta\n",
    "    \n",
    "    \n",
    "    # initialize delta array for each value of r\n",
    "    '''\n",
    "    for r in range(0,L):\n",
    "        if r==L-1:\n",
    "            deltas[r] = deltaL\n",
    "        else:\n",
    "            deltas[r] = [] # sample value \n",
    "    '''\n",
    "    \n",
    "    # example , delta = [[],[],deltaL] , here [] is to be set\n",
    "    # delta[r] => k[r]\n",
    "    \n",
    "    # now the backpropagation part, the mighty equation \n",
    "    \n",
    "    for r in reversed(range(1,L,1)): # for each layer of L \n",
    "        for j in range(0,k[r-1]): # for every j th node of layer r-1 j = 0\n",
    "            e_j = 0 # initialize error with zero for j th node\n",
    "            for k in range(0,self.k[r]): # for each node in layer r k = 0,1,2,3\n",
    "                e_j += delta[r][k]*W[r][k][j+1] # delta for each node in layer r and j th value of k th row in W[r]\n",
    "            delta[r-1][j] = e_j*derivative(v[r-1][j]) # delta for j th node in layer r - 1\n",
    "    \n",
    "    # update weights\n",
    "    \n",
    "    for r in range(0,L):\n",
    "        for j in range(0,k[r]):\n",
    "            # calculate delta w\n",
    "            # wrj (new) = wrj (old) + del(wrj)\n",
    "            pass\n",
    "    \n",
    "    pass\n",
    "\n",
    "def calculate_cost(y_m,Y):\n",
    "    \n",
    "    total_sum = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # calculate euclidian distance between y_m[i] and Y[i]\n",
    "        # add this to total sum\n",
    "        pass\n",
    "        \n",
    "    return total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97408734,  0.89900514, -2.37149757,  3.497146  ,  4.18775846],\n",
       "       [-2.70518764, -4.54437507, -1.56037821,  2.44574707, -1.6786656 ],\n",
       "       [-3.50647958, -2.10739222, -2.35902223,  4.70142255,  1.1706795 ],\n",
       "       [ 1.20803539,  3.33997747,  4.0443813 , -0.26735345, -3.61041569]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-5,5,size=(4,5)) # this will generate a shape of row->4 and column->5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'k' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-2dea3d8c3db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-425ba9fd997e>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(v, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each layer of L in reverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for every j th node of layer r-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0me_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# initialize error with zero for j th node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each node in layer r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'k' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Learn block\n",
    "\n",
    "MAXEPOCH = 1000\n",
    "\n",
    "for epoch in range(MAXEPOCH):\n",
    "    \n",
    "    y_m = [] # actual output of neural network\n",
    "     \n",
    "    for instance,target in zip(X,Y):\n",
    "        # forward propagation\n",
    "        v,y = forward_propagation(instance,target)\n",
    "        # backpropagation\n",
    "        back_propagation(v,y)\n",
    "        \n",
    "    cost = calculate_cost(y_m,Y)\n",
    "    \n",
    "    if(cost<threshold):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "for instance,target in zip(X_test,Y_test):\n",
    "    v,y = forward_propagation(instance,target)\n",
    "    \n",
    "    # if arg max of y[L] is equal to arg max of target increment count\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
